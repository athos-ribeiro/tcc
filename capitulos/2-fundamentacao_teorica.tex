\chapter{Fundamentação Teórica}\label{fundamentacao_teorica}

Este capitulo contextualiza o presente trabalho, apresentando definições de conceitos que serão utilizados em outros capítulos. Inicialmente define-se análise estática de código fonte e apresentam-se exemplos métricas\index{métrica} para um contexto de segurança. Em seguida, são apresentadas algumas ferramentas\index{ferramenta} para análise estática, bem como a definição exemplos de bases de dados de vulnerabilidades\index{vulnerabilidade}. Por fim, discutem-se trabalhos relacionados à análise estática de segurança de código fonte e avaliações\index{avaliações} deste tipo de ferramenta.

\section{Análise Estática de Código}\label{fundamentacao_teorica:analise_estatica_de_codigo}

Metade dos defeitos\index{defeito} de segurança de software são introduzidos à nível de código-fonte \cite{vadim}. Em tal contexto, software pode ser verificado através da análise dinâmica ou da análise estática \cite{concolic}.

Técnicas de análise dinâmica são baseadas na análise de software realizada através da execução do programa analisado, em um processador real ou virtual \cite{concolic}, e podem encontrar defeitos\index{defeito} ao longo da fase de testes\index{teste}. Mas para tal necessitam que o código esteja em um estado em que possa ser executado e instalado. Além disso, apenas os defeitos\index{defeito} que se encontram no caminho de execução que as entradas dos testes\index{teste} fornecem podem ser encontrados \cite{harvard}.

Análise estática de código é a análise de programas de computador sem a necessidade de executar tais programas, geralmente feita por ferramentas\index{ferramenta} externas, de forma automatizada \cite{kannavara}. À tais ferramentas\index{ferramenta} dá-se o nome de ferramenta de análise estática de código fonte, ou simplesmente ferramenta de análise estática. Segundo \cite{secure_programming}, qualquer ferramenta que analisa código sem o executar está realizando análise estática de código.

Ferramentas\index{ferramenta} de análise estática podem inferir informações  acerca de um software sem a necessidade de executar o mesmo, tornando-as ferramentas\index{ferramenta} adequadas para detecção de fraquezas\index{fraqueza} no código \cite{vadim}. Deste modo, tais ferramentas\index{ferramenta} se mostram um ponto razoável para que se possa buscar maior qualidade final do software \cite{sa_spec}.

\textbf{Fraquezas\index{fraqueza}} são defeitos\index{defeito} de software provenientes do design, implementação ou outras áreas do ciclo de desenvolvimento do software. Em condições específicas, fraquezas\index{fraqueza} podem contribuir para a introdução de \textbf{vulnerabilidades\index{vulnerabilidade}} no software. Quando ativadas, estas vulnerabilidades\index{vulnerabilidade} (manifestações de fraquezas\index{fraqueza}) resultam em falhas de sistema, ou simplesmente falhas.
\begin{citacao}
Muitas vezes os termos fraqueza e vulnerabilidade são confundidos ou as pessoas discordam de suas definições. As ferramentas\index{ferramenta} de análise estática buscam por fraquezas\index{fraqueza} no código, quando as pessoas, em grande parte das vezes, gostariam que elas buscassem por vulnerabilidades\index{vulnerabilidade}\footnote{por Steven M. Christey, engenheiro de segurança da informação do MITRE em uma lista de discussão sobre segurança web disponível em \url{http://lists.webappsec.org/pipermail/websecurity_lists.webappsec.org/2011-November/008118.html}}.
\end{citacao}

O MITRE é uma organização sem fins lucrativos que opera centros de pesquisa e desenvolvimento patrocinados pelo governo federal dos Estados Unidos da América. Uma das iniciativas do MITRE, patrocinada por um projeto de métricas\index{métrica} de qualidade de software e avaliação\index{avaliação} de ferramentas\index{ferramenta} (SAMATE - Software Assurance Metrics and Tool Evaluation), visa elaborar e manter um catálogo de fraquezas\index{fraqueza} de software, onde as fraquezas\index{fraqueza} de software conhecidas recebem uma definição formal e um número identificador. Tal catálogo de fraquezas\index{fraqueza} é conhecido como Common Weakness Enumeration ou CWE\footnote{\url{cwe.mitre.org}}\index{CWE}, e tem sido amplamente adotado para referenciar fraquezas\index{fraqueza} de software, tanto na indústria quanto na academia.

Consideraremos fraquezas\index{fraqueza} de software apenas os defeitos\index{defeito} que estiverem mapeadas como fraquezas\index{fraqueza} em forma de CWE\index{CWE}. Sendo assim, chamaremos de defeito (ou \textit{bug}) as fraquezas\index{fraqueza} do software.

Existem diferentes técnicas de análise estática de código-fonte, que variam desde simples análises\index{análise} léxicas, análises\index{análise} de fluxo de dados à análise de grafos de fluxo de controle. Ao apontar defeitos\index{defeito} no código, uma ferramenta de análise estática pode ou não cometer erros quando analisando determinado trecho de código, dada a natureza do defeito ali presente (ou ausência de defeitos\index{defeito}) e à técnica de análise empregada. Algumas técnicas são menos sofisticadas e estão mais suscetíveis a erros do que outras \cite{harvard}. Ainda assim, cada uma das técnicas de análise estática tem suas limitações \cite{pascal}, o que justifica o uso de e a busca por diferentes técnicas de análise.

Conforme a terminologia utilizada em \cite{sa_spec}, chama-se \textbf{falso positivo}\index{falso positivo}, ou alarme falso, um trecho de código apontado como defeituoso por uma ferramenta de análise estática que, na verdade, não se trata de um defeito, ou seja, a ferramenta cometeu um erro ao apontar um trecho de código não defeituoso como defeituoso. Chama-se \textbf{falso negativo}\index{falso negativo} um defeito no código fonte, que se enquadra na categoria de defeitos\index{defeito} detectados pela ferramenta que analisa o código em questão, que não foi detectado pela ferramenta, ou seja, a ferramenta cometeu um erro ao não apontar um trecho de código defeituoso.

Uma ferramenta ideal não cometeria erros, apontando todos os defeitos\index{defeito} do código (mesmo que apenas em uma dada categoria) e nunca apontando um trecho de código correto como defeituoso (a ferramenta não comete falsos positivos\index{falso positivo} nem falsos negativos\index{falso negativo}). Trabalhos anteriores mostram que é impossível, mesmo em teoria, produzir tal ferramenta \cite{sa_spec} devido ao Teorema de Rice \cite{rice}, que prova que para qualquer propriedade de software que não seja trivial, não existe algoritmo capaz de decidir se um programa qualquer possui tal propriedade, de modo que buscam-se aproximações para a solução de problemas ou tomadas de decisões relacionadas à análise estática. É possível, porém, produzir ferramentas\index{ferramenta} capazes de cometer apenas um dos tipos de erro (falsos positivos\index{falso positivo} ou falsos negativos\index{falso negativo}). À análise estática que não gera falsos negativos\index{falso negativo}, ou seja, aponta todos os defeitos\index{defeito} no código, embora aponte alguns trechos como defeituosos quando os mesmos não são defeitos\index{defeito} de fato (ou seja, gera falsos positivos\index{falso positivo}) mas sem deixar de apontar nenhum defeito real, chamamos \textbf{análise correta}. À análise estática que não gera falsos positivos\index{falso positivo}, ou seja, todos os possíveis defeitos\index{defeito} apontados por ela são de fato trechos de código defeituosos, embora deixe de apontar alguns defeitos\index{defeito} presentes no código (ou seja, gera falsos negativos\index{falso negativo}), chama-se \textbf{análise completa}\footnote{Ao longo do SATE V, as definições de análise completa e análise correta foram contestadas por alguns dos participantes, porém, as definições aqui apresentadas são as definições atualmente utilizadas pelo NIST\index{NIST}}. Por fim, pode-se relacionar os termos com os Teoremas de Incompletude \cite{godel}, de modo que não é possível realizar uma análise completa e correta.

\section{Métricas\index{métrica} e Métricas\index{métrica} de Segurança}

Em \cite{nsa}, o CAS (Center for Assured Software) da Agência de Segurança Nacional (NSA – National Security Agency), centro de pesquisa criado para aumentar o nível de confiança de que os software utilizados pelo Departamento de Defesa dos Estados Unidos (DoD – Department of Defense) estão livres de vulnerabilidades\index{vulnerabilidade} \cite{nsa}, apresenta um conjunto de métricas\index{métrica} para que se aplique sua metodologia. Das métricas\index{métrica} apresentadas, algumas são de propósito geral, ou seja, independentes da natureza da suíte de testes\index{teste} ou mesmo da metodologia de testes\index{teste} empregada na análise, desde que sejam conhecidos todos os defeitos\index{defeito} presentes no código a ser analisado. Ao realizar seu \textit{workshop} de avaliação\index{avaliação} de ferramentas\index{ferramenta}, o SATE, o NIST\index{NIST} utiliza este conjunto de métricas\index{métrica} descrito pelo CAS para suas análises\index{análise}, como notado em \cite{sate_iv}. Essas métricas\index{métrica} são:
\begin{description}
    \item[Positivos\index{positivo}:] Quantidade total de defeitos\index{defeito} reportados pela ferramenta que são de fato defeitos\index{defeito} no código-fonte analisado.

    \item[Falsos Positivos\index{falso positivo}:] Quantidade total de defeitos\index{defeito} reportados pela ferramenta que não são defeitos\index{defeito} no código-fonte analisado.

    \item[Falsos Negativos\index{falso negativo}:] Quantidade de defeitos\index{defeito} presentes no código fonte que não foram reportados pela ferramenta.

      Note que o conjunto que contém todos os Positivos\index{positivo} e todos os Falsos Negativos\index{falso negativo}, é o conjunto de todos os defeitos\index{defeito} presentes no código-fonte. Tal conceito foi amplamente utilizado para as análises\index{análise} da \textit{Ockham\index{ockham} Criteria}\footnote{\url{http://samate.nist.gov/SATE5OckhamCriteria.html}} realizada na quinta edição do SATE\footnote{\url{http://samate.nist.gov/SATE5.html}}, pelo NIST\index{NIST}, cujos resultados ainda não foram publicados. Chamaremos tal conjunto de Trechos Defeituosos, (Equação \eqref{eq1}),
\begin{equation}\label{eq1}
TD = P + FN,
\end{equation}
  onde $TD$ é o número de Trechos Defeituosos, $P$ o número de Positivos\index{positivo} e $FN$ o número de Falsos Negativos\index{falso negativo}.

  No contexto da \textit{Ockham\index{ockham} Criteria} do SATE V do NIST\index{NIST}, considera-se um defeito apenas se o mesmo for uma fraqueza, ou seja, um defeito só é considerado quando formalizado de acordo com o CWE\index{CWE}, como mencionado   na Seção \nameref{fundamentacao_teorica:analise_estatica_de_codigo} do presente trabalho. Isso implica que a Equação \eqref{eq1} pode não ser verdadeira, dado que podem existir defeitos\index{defeito} reportados pela ferramenta que não estejam mapeados em forma de CWE\index{CWE}, ou ainda, que estejam sujeitos à interpretação das CWE\index{CWE}, que como mencionado em \cite{yan} e notado ao longo dos trabalhos  da \textit{Ockham\index{ockham} Criteria}, nem sempre são definidas com precisão.

\item[Precisão:] Número de defeitos\index{defeito} reais reportados dividido pelo número total de defeitos\index{defeito} reportados, que inclui os não-defeitos\index{defeito} incorretamente reportados como defeitos\index{defeito} ($FP$).  Ela pode ser representada pela Equação \eqref{eq2},
\begin{equation}\label{eq2}
  P_r = \frac{P}{P + FP}
\end{equation}
  onde $P_r$ é a precisão, $P$ o número de Positivos\index{positivo} e $FP$ o número de Falsos Positivos\index{falso positivo}.

  A precisão indica a capacidade da ferramenta de identificar defeitos\index{defeito}. Note que a precisão é indefinida quando uma ferramenta não reporta erros \cite{nsa}, pois $P = 0$, e $FP = 0$, de modo que $P_r = \frac{0}{0}$. Quando definida, $0 \leq Pr \leq 1$.

\item[Corretude:] Corresponde à fração de defeitos\index{defeito} reais reportados pela ferramenta. É definida como o número de defeitos\index{defeito} reais reportados dividido pelo número total de defeitos\index{defeito} existentes no código, que inclui os defeitos\index{defeito} reais que não foram reportados ($FN$), conforme apresenta a Equação \eqref{eq3},
\begin{equation}\label{eq3}
  C_r = \frac{P}{P + FN},
\end{equation}
  onde $C_r$ é a Corretude, $P$ o número de positivos\index{positivo} e $FN$ o número de falsos negativos\index{falso negativo}, ou, em casos onde a Equação \eqref{eq1} for verdadeira, de acordo com a Equação \eqref{eq4}.
\begin{equation}\label{eq4}
  C_r = \frac{P}{TD}.
\end{equation}

  Valores altos de corretude indicam que a ferramenta identifica grande parte dos defeitos\index{defeito} corretamente. Note que $0 \leq C_r \leq 1$.

  Uma ferramenta que apresenta Corretude igual a 1 (um) na maior parte das análise é, possivelmente, uma ferramenta de análise estática correta. Uma ferramenta é considerada não correta se sua Corretude  menor do que 1 para uma dada análise.

  \item[F-score:] Média harmônica da Correção e Precisão. O F-score auxilia no julgamento de uma ferramenta de análise estática verificando a quantidade de defeitos\index{defeito} capturados e quanto ruído foi gerado no processo (falsos positivos\index{falso positivo}). Utiliza-se a media harmônica de modo a prover pontuação justa para ferramentas\index{ferramenta} que com bons resultados tanto para Corretude quanto para Precisão (Uma ferramenta com Corretude alta e Precisão baixa não teria melhores resultados que uma ferramenta mediana para ambas as métricas\index{métrica}). O cálculo do F-score é apresentado na Equação \eqref{eq5},
\begin{equation}\label{eq5}
  F_s = 2\left(\frac{P \times C_r}{P + C_r}\right)
\end{equation}
onde $F_s$ é o F-score, $P$ a precisão e $C_r$ a Corretude.
\end{description}

  Existem outras métricas\index{métrica} que são relevantes apenas para análises\index{análise} que utilizem suítes de teste como a proposta pelo CAS em \cite{nsa}, também descrita em \cite{juliet}. A mesma se trata de uma suíte de testes\index{teste} composta de casos de teste artificiais ou sintéticos, que são definidos posteriormente, na Seção \nameref{fundamentacao_teorica:bases_de_testes_de_vulnerabilidades}.

  \section{Ferramentas\index{ferramenta} de Análise Estática de Código}

  Existem diversas ferramentas\index{ferramenta} para análise estática de código-fonte. Algumas delas se especializam em diversos tipos de análise, dificultando qualquer esforço de classificá-las como ferramentas\index{ferramenta} voltadas à segurança de código-fonte ou não. O grupo de trabalho SAMATE do  NIST\index{NIST} mantém uma lista com ferramentas\index{ferramenta} de análise estática voltada à segurança. Esta lista é de acesso público, irrestrito e pode ser acessada em \url{http://samate.nist.gov/index.php/Source_Code_Security_Analyzers.html}. Embora a lista seja aberta, a mesma inclui ferramentas\index{ferramenta} de diferentes licenças, onde algumas são Livres\footnote{O conceito de Software Livre é formalmente definido em \url{https://www.gnu.org/philosophy/free-sw.html}} e outras, proprietárias, de modo que nem todas as ferramentas\index{ferramenta} estão facilmente disponíveis para uso, testes\index{teste} ou avaliações\index{avaliações}.

  Note que compiladores que indicam \textit{warnings} ao longo do processo de compilação estão indicando possíveis defeitos\index{defeito} que podem levar a vulnerabilidades\index{vulnerabilidade}, de modo que tais compiladores podem ser considerados ferramentas\index{ferramenta} de análise estática no presente contexto.


  \section{Bases de Testes\index{teste} de Vulnerabilidades\index{vulnerabilidade}}\label{fundamentacao_teorica:bases_de_testes_de_vulnerabilidades}

  Para que se possam avaliar ferramentas\index{ferramenta} de análise estática de código fonte, é necessário que se possua o software à disposição para que as ferramentas\index{ferramenta} sob avaliação\index{avaliação} possam analisá-lo \cite{nsa}. Ao software utilizado para avaliação\index{avaliação} de ferramentas\index{ferramenta} de análise estática dá-se o nome de \textbf{caso de teste}. Um conjunto de casos de teste é chamado de \textbf{suíte de testes\index{teste}}.

  Conforme \cite{nsa}, existem dois tipos de caso de teste: \textbf{caso de teste natural}, quando utiliza-se um software qualquer, de propósito qualquer, como caso de teste para ferramentas\index{ferramenta} de análise estática e \textbf{casos de teste artificial}, ou como chamados em \cite {juliet}, caso de teste sintético, quando estes são criados especificamente para testar ferramentas\index{ferramenta} de análise estática de código fonte.

  Para gerar casos de teste sintéticos é necessário que se insiram defeitos\index{defeito} no código de forma proposital. Isto facilita a avaliação\index{avaliação} da ferramenta, uma vez que a localização de todos os defeitos\index{defeito} do código a ser analisado, bem como sua natureza, são conhecidas\footnote{Exceto quando, como mencionado em \cite{pascal}, existam bugs\index{bug} no próprio caso de teste}. Casos de teste naturais são nada mais que o código fonte de software qualquer. Estes podem ser facilmente obtidos graças às licenças de Software Livre como a \textit{GNU General Public License} (GPL),  na qual um de seus termos implica na distribuição do código-fonte juntamente com os executáveis\footnote{\url{http://www.gnu.org/licenses/gpl-3.0.en.html}}.

  Há ainda \textbf{casos de teste híbridos}, onde injetam-se defeitos\index{defeito} em software conhecidos para que se possa aumentar o grau de complexidade do caso de teste (que é baixo em casos de teste artificiais) mantendo um maior controle sob os defeitos\index{defeito} presentes no código (que é baixo em casos de teste naturais). Um exemplo de casos de teste híbridos é a suíte de testes\index{teste} STONESOUP (\textit{Securely Taking On New Executable Software of Uncertain Provenance}), onde a IARPA (\textit{Intelligence Advanced Research Projects Activity}) juntamente com uma empresa contratada selecionou alguns software de licenças livres como grep\footnote{\url{https://www.gnu.org/software/grep/}} e injetaram diversas CWE\index{CWE} em seus códigos\index{código}-fonte, gerando uma Suíte de Testes\index{teste} com mais de 7.000 Casos de Teste. Embora o projeto tenha sido iniciado com um objetivo diferente\footnote{Para mais detalhes sobre o projeto STONESOUP, ver \url{http://www.iarpa.gov/index.php/research-programs/stonesoup}}, os casos de testes\index{teste} gerados foram transferidos para o NIST\index{NIST}, de modo que o mesmo pudesse disponibilizá-los ao público.

  Ao se selecionar o tipo de caso de teste a ser utilizado devem-se considerar os objetivos da avaliação\index{avaliação} em questão. Kratkiewicz \cite{harvard} propõe que uma ferramenta de análise estática verdadeiramente útil deve ter uma boa taxa de detecção de defeitos\index{defeito} quando  analisar programas grandes do mundo real\footnote{Infere-se aqui que um programa grande seja qualquer software de código não trivial e um programa do mundo real seja qualquer software utilizado em produção que gere algum valor ao usuário final}. O autor ainda sugere que a não detecção de um defeito específico em um caso de teste sintético por uma ferramenta de análise estática implica na não detecção (a partir de análise com a mesma ferramenta de análise estática) de um defeito similar em um código mais complexo, do mundo real (visto que o primeiro trata-se de um caso pontual, simplificado). Porém, não pode-se afirmar que, caso tal defeito seja detectado no caso de teste sintético, um defeito similar a ele também será detectado no código mais complexo, de mundo real (fato que remete ao Teorema de Rice \cite{rice} quando à impossibilidade de descrever comportamentos não-triviais de programas, e que pode ser justificado ao afirmar que análise estática trabalha com aproximações). Por fim \cite{harvard} afirma que um caso de testes\index{teste} deve ser estruturado de modo a prover informações precisas quanto a detecção, falsos positivos\index{falso positivo} e casos em que a ferramenta se confunde. Para o último cenário, apontam-se casos de testes\index{teste} com duas versões do mesmo código, onde uma delas possui um defeito e a outra não.

  Pascal Cuoq \cite{pascal} afirma que um caso de teste deve possuir no máximo um comportamento indefinido\footnote{entenda por defeito}, dado que com mais de um comportamento indefinido em um caso de teste, pode haver mal julgamento da ferramenta de análise estática, como no caso de a ferramenta terminar sua execução em determinado fluxo de dados ao encontrar um defeito crítico no mesmo. Neste caso, um defeito inserido posteriormente sequer seria analisado. Sendo assim, nota-se que Pascal Cuoq nitidamente sugere que se usem casos de teste sintéticos, mas alerta que os mesmos não são capazes de medir o quão bem um programa de análise estática se sairia em produção, dado que neste caso tem-se um ambiente menos controlado, com inúmeras possibilidades de entrada de uma só vez.

  Existem algumas iniciativas para coletar e redistribuir casos de testes\index{teste} para análise estática de código fonte, de modo a auxiliar pesquisadores e estudantes interessados em análise estática, bem como proporcionar bases de testes\index{teste} para desenvolvedores de ferramentas\index{ferramenta} de análise estática de modo a melhorar a qualidade deste tipo de software. Dentre tais iniciativas, destacam-se dois programas norte americanos, o SARD  – \textit{Software Assurance Reference Dataset}, anteriormente conhecido como SRD – \textit{SAMATE Reference Dataset}\footnote{O SARD teve seu nome alterado em 2014, pois o NIST\index{NIST}, seu mantenedor, já possui um outro projeto com o nome de SRD}, que visa disponibilizar casos e suítes de testes\index{teste} para estudo de fraquezas\index{fraqueza} e vulnerabilidades\index{vulnerabilidade} de software, para que indústria e academia possam melhor entendê-los, conduzir estudos e produzir ferramentas\index{ferramenta} com maior qualidade\footnote{\url{http://samate.nist.gov/index.php/SARD.html}} e o SWAMP (\textit{Software Assurance Marketplace}).

  O SARD é uma aplicação web com um banco de dados que armazena e disponibiliza, livre de cobranças, casos de teste para qualquer interessado. Para obter os casos de teste do SARD basta acessá-lo em \url{http://samate.nist.gov/SARD/}. Dentre os casos e suítes de testes\index{teste} disponibilizados pelo SARD, estão o Juliet, em sua versão 1.2, elaborado pelo CAS, e o STONESOUP, em sua terceira versão, elaborado pela IARPA, agência de pesquisa do governo norte-americano. O SAMATE obtém seus casos de teste para o SARD através de doações de órgãos do governo norte-americano ou de outros países, universidades, centros de pesquisa ou qualquer outra parte interessada em disponibilizar casos de teste de vulnerabilidades\index{vulnerabilidade} para as comunidades interessadas em análise estática voltada para segurança. No momento que este trabalho foi elaborado, o SARD possuía casos de teste em diferentes linguagens, incluindo C, Java e PHP.

  A suíte de testes\index{teste} Juliet for criada pelo CAS para que o mesmo pudesse melhor selecionar as ferramentas\index{ferramenta} de análise estática para o DoD. Os resultados e metodologia desses trabalhos de seleção podem ser vistos em \cite{nsa}.

  O SWAMP é um projeto do governo norte-americano financiado pelo DHS que provê uma infraestrutura para análise estática de código fonte que inclui uma plataforma para análise estática de projetos em diferentes linguagens, casos de teste fornecidos pela comunidade (assim como o SARD, do NIST\index{NIST}) e programas para análise estática de código fonte de licenças livres disponíveis para download. O objetivo do projeto é difundir o conhecimento e fomentar a pesquisa acerca de análise estática, além de auxiliar empresas e desenvolvedores a melhorar a qualidade de seu software, diminuindo a quantidade de bugs\index{bug} dos mesmos. O projeto também visa auxiliar desenvolvedores de ferramentas\index{ferramenta} de análise estática, dado que provê diversos casos de teste. Toda a infraestrutura do projeto é disponibilizada de forma aberta\footnote{SWAMP: \url{continuousassurance.org} e \url{www.mir-swamp.org}}.                   

  \section{Trabalhos Correlatos}

  Embora a análise estática de código fonte não seja uma  área nova na computação – note que por definição, a própria etapa de compilação de código pode ser considerada análise estática de código – sua aplicação em segurança de código fonte é recente na academia e ainda não é consolidada na indústria \cite{johnson2013don}.

  O governo dos Estados Unidos tem investido grandes quantidades de recursos em programas de segurança da informação. O \textit{National Institute of Standards and Technology} (NIST\index{NIST}) possui um grupo de trabalho financiado pelo departamento de segurança nacional (\textit{Department of Homeland Security} – DHS) voltado ao estudo de métricas\index{métrica} de qualidade de software e avaliação\index{avaliação} de ferramentas\index{ferramenta} (para garantia de qualidade). O SAMATE – \textit{Software Assurance Metrics And Tool Evaluation} –, como é chamado, conduz estudos em métricas\index{métrica} de software e análise estática de código fonte. Seus dois projetos principais são o SARD e o SATE – \textit{Static Analysis Tool Exposition}, um \textit{workshop} realizado periodicamente na sede do NIST\index{NIST} em Gaithersburg, MD nos Estados Unidos\footnote{\url{http://samate.nist.gov/SATE5Workshop.html}} que reúne indústria e academia para discutir o estado da arte de programas de análise estática de código-fonte, bem como apresentar resultados e comparações de avaliações\index{avaliações} de ferramentas\index{ferramenta} de análise estática realizadas ao longo de meses pelo SAMATE (não há prazo nem uma janela de tempo específica que o SAMATE leva para analisar as ferramentas\index{ferramenta} participantes do SATE, dado que tais fatores dependem do tamanho da equipe, quantidade de ferramentas\index{ferramenta} avaliadas e tamanho das suítes de testes\index{teste} utilizadas, bem como da própria performance das ferramentas\index{ferramenta} avaliadas).

  O SATE teve sua quinta edição realizada ao longo do ano de 2014 e ainda não teve (até a ocasião da escrita deste trabalho) seus resultados e artigos publicados, dado o número elevado de participantes e à inclusão de novas categorias, como a \textit{Ockham\index{ockham} Criteria} para ferramentas\index{ferramenta} de análise correta\footnote{\url{http://samate.nist.gov/SATE5OckhamCriteria.html}}. A quarta edição do SATE foi realizada em 2013 e seus resultados e artigos referentes ao mesmo podem ser obtidos no sítio do NIST\index{NIST}\footnote{\url{http://samate.nist.gov/index.php/SAMATE_Publications.html}}.


  O MITRE, organização sem fins lucrativos que opera centros de pesquisa e desenvolvimento patrocinados pelo governo federal dos Estados Unidos, iniciou estudos de vulnerabilidade de software no ano de 1999, patrocinado pelo próprio SAMATE. Tais estudos resultaram em definições formais de vulnerabilidades\index{vulnerabilidade} conhecidas, presentes em software de produção como o Microsoft Windows\footnote{\url{http://www.cvedetails.com/vulnerability-list/vendor_id-26/product_id-26434/Microsoft-Windows-8.1.html}} e servidores web como o Apache\footnote{\url{http://www.cvedetails.com/vulnerability-list/vendor_id-45/Apache.html}}. Tal projeto é conhecido como CVE\index{CVE} – \textit{Common Vulnerabilities and Exposures}, e pode ser acessado em \url{https://cve.mitre.org/}. O NIST\index{NIST} também mantém uma base de dados com catálogos para as CVE\index{CVE}, o NVD – \textit{National Vulnerability Database}, que pode ser acessado em \url{https://nvd.nist.gov/}. De uma iniciativa para atomizar e melhor compreender as vulnerabilidades\index{vulnerabilidade} definidas como CVE\index{CVE}, o MITRE criou um projeto para definir as fraquezas\index{fraqueza} de software. Estas são as CWE\index{CWE} – \textit{Common Weakness Enumeration}. Embora haja diversos problemas com as definições de CWE\index{CWE} \cite{yan}, não há outra definição formal comum para fraquezas\index{fraqueza} de software com bons níveis de aceitação, de modo que uma mesma fraqueza pode ser apontada com diferentes descrições por diferentes ferramentas\index{ferramenta} de análise estática de código fonte. Em \cite{clustering} sugere-se o uso de ferramentas\index{ferramenta} para agregar relatórios relacionados a um mesmo defeito com o uso de ferramentas\index{ferramenta} específicas para tal.

  É uma pratica comum que distribuições de sistemas operacionais GNU/Linux ofereçam pacotes de software para seus usuários. Sendo assim, é uma preocupação válida de seus times de garantia de qualidade que tais pacotes não contenham defeitos\index{defeito} ou que a quantidade de defeitos\index{defeito} em seus pacotes seja minimizada ao longo do tempo. Existem iniciativas para promover a análise estática de código fonte de tais pacotes por estes times de garantia de qualidade. Dentre elas, destacam-se o projeto Debile\footnote{\url{http://debile.debian.net/about}}, da distribuição Debian\footnote{\url{debian.org}} e o grupo de interesse em análise estática de código fonte\footnote{\url{https://fedoraproject.org/wiki/StaticAnalysis}} do projeto Fedora\footnote{\url{fedoraproject.org}}.

  O primeiro trata-se de uma ferramenta em desenvolvimento para análise de código fonte de todos os pacotes da distribuição ao longo do processo de construção dos mesmos. A ferramenta utiliza ferramentas\index{ferramenta} de análise estática de código fonte de licença aberta como o flake8\footnote{\url{https://pypi.python.org/pypi/flake8}} e pylint\footnote{\url{http://www.pylint.org/}}, porém ainda não existem critérios para a seleção das ferramentas\index{ferramenta}. O último consiste em um grupo de interesse com diversas iniciativas para aproximar o projeto Fedora da análise estática de código fonte. Dentre elas, destaca-se a construção de uma ferramenta para análise estática de código fonte de pacotes ao longo de testes\index{teste} de construção dos mesmos. Esta ferramenta é similar ao Debile, porém  apenas para construções locais, para testes\index{teste} das mesmas e não para produção. Nesse também não há critérios para seleção das ferramentas\index{ferramenta} que realizam as análises\index{análise}. Ambas as ferramentas\index{ferramenta} utilizam de uma mesma linguagem em comum para produzirem seus relatórios de defeitos\index{defeito}, o firehose\footnote{\url{https://github.com/fedora-static-analysis/firehose}}. O firehose é um módulo escrito em linguagem Python pelo grupo de interesse em análise estática do projeto Fedora que define uma linguagem XML para padronização de relatórios de análise estática de código fonte.

  O CAS da NSA publicou em 2012 uma metodologia para avaliação\index{avaliação} de ferramentas\index{ferramenta} de análise estática de código-fonte \cite{nsa}. Esta metodologia se assemelha ao trabalho aqui apresentado, que utiliza conceitos apresentados na mesma para generalizar métodos de avaliação\index{avaliação} de ferramentas\index{ferramenta}, para que, diferente da metodologia apresentada pelo CAS, o processo seja independente de ferramentas\index{ferramenta} específicas, como é o caso apresentado pelo CAS, cuja metodologia depende de suíte de testes\index{teste}, métricas\index{métrica} e linguagem de saída de defeitos\index{defeito} reportados específicas. As influências dos trabalhos apresentados pelo CAS encontradas neste trabalho se dão pelo fato de o NIST\index{NIST} basear parte de seus trabalhos apresentados no SATE na metodologia do CAS. Note que este trabalho busca apresentar uma abordagem independente de ferramentas\index{ferramenta} e linguagens, diferente dos trabalhos aqui mencionados.

